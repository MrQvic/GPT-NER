{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "classifier = pipeline(\"ner\", model=\"stulcrad/CNEC_2_0_robeczech-base\")\n",
    "str_ = \"Jste světa znalý muž a víte stejně dobře jako já , že souvislost mezi současnými krutostmi v Jihovýchodní Asii a tou novou bankovní pobočkou hned vedle obchoďáku Zátoka je přímá a bezprostřední ; byl z toho už vzteklý jak uvázaný pes , protože zájemci o hodiny mu úplně narušili jeho denní režim a on si nemohl po obědě ani zdřímnout .\"\n",
    "classifier(str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_[162:168]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "def combine_adjacent_name_tokens(results):\n",
    "    \"\"\"\n",
    "    Spojí navazující tokeny jmen do celých jmen.\n",
    "    \n",
    "    Args:\n",
    "        results: Seznam tokenů z NER modelu.\n",
    "    \n",
    "    Returns:\n",
    "        Seznam slovníků obsahujících spojená jména.\n",
    "    \"\"\"\n",
    "    # Typy tokenů pro jména\n",
    "    person_types = ['pf', 'ps', 'pp', 'pm', 'pc', 'pd' ]\n",
    "    \n",
    "    combined_names = []\n",
    "    current_name = None\n",
    "    last_index = -1\n",
    "    \n",
    "    # Seřazení výsledků podle indexu\n",
    "    sorted_results = sorted(results, key=lambda x: x['index'])\n",
    "    \n",
    "    for token in sorted_results:\n",
    "        # Rozdělení entity na typ\n",
    "        entity_type = token['entity'].split('-')[1] if '-' in token['entity'] else \"\"\n",
    "        if entity_type in person_types:\n",
    "            if current_name is None or token['index'] != last_index + 1:\n",
    "                # Nové jméno\n",
    "                if current_name:\n",
    "                    combined_names.append(current_name)\n",
    "                current_name = {\n",
    "                    'start': token['start'],\n",
    "                    'end': token['end']\n",
    "                }\n",
    "            else:\n",
    "                # Přidání dalšího tokenu\n",
    "                current_name['end'] = token['end']\n",
    "            \n",
    "            last_index = token['index']\n",
    "    \n",
    "    # Přidání posledního jména\n",
    "    if current_name:\n",
    "        combined_names.append(current_name)\n",
    "        \n",
    "    return combined_names\n",
    "\n",
    "def extract_and_mark_names(text, results):\n",
    "    \"\"\"\n",
    "    Ohraničí nalezená jména značkami @@ a ##.\n",
    "    \n",
    "    Args:\n",
    "        text: Původní text.\n",
    "        results: Výstupy z NER modelu.\n",
    "    \n",
    "    Returns:\n",
    "        Text s označenými jmény.\n",
    "    \"\"\"\n",
    "    \n",
    "    combined = combine_adjacent_name_tokens(results)\n",
    "    combined.sort(key=lambda x: x['start'])\n",
    "    \n",
    "    marked_text = \"\"\n",
    "    last_end = 0\n",
    "    \n",
    "    # Přidání @@## k nalezeným jménům\n",
    "    for name in combined:\n",
    "        marked_text += text[last_end:name['start']]\n",
    "        \n",
    "        original_name = text[name['start']:name['end']]\n",
    "        marked_text += \"@@\" + original_name + \"##\"\n",
    "        \n",
    "        last_end = name['end']\n",
    "    \n",
    "    marked_text += text[last_end:]\n",
    "    \n",
    "    return marked_text\n",
    "\n",
    "def process_dataset_with_model(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Zpracuje dataset pomocí modelu a uloží označené kontexty do výstupního souboru.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Cesta k vstupnímu JSON souboru.\n",
    "        output_file: Cesta k výstupnímu textovému souboru.\n",
    "    \"\"\"\n",
    "    # Načtení modelu\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"stulcrad/CNEC_2_0_robeczech-base\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"stulcrad/CNEC_2_0_robeczech-base\")\n",
    "    classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    # Načtení datasetu\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Zpracování a uložení\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            context = item[\"context\"]\n",
    "            results = classifier(context)\n",
    "            marked_context = extract_and_mark_names(context, results)\n",
    "            f.write(marked_context + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"mrc-ner.test.154.json\"\n",
    "    output_file = \"hokuspokus154.txt\"\n",
    "    process_dataset_with_model(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-gu', 'score': 0.9969068, 'index': 1, 'word': 'ĠO', 'start': 0, 'end': 1}, {'entity': 'I-gu', 'score': 0.9969133, 'index': 2, 'word': 'LOMOUC', 'start': 1, 'end': 7}, {'entity': 'B-ty', 'score': 0.9982508, 'index': 3, 'word': 'Ġ1920', 'start': 8, 'end': 12}, {'entity': 'B-ic', 'score': 0.99747854, 'index': 5, 'word': 'ĠAR', 'start': 15, 'end': 17}, {'entity': 'I-ic', 'score': 0.9979342, 'index': 6, 'word': 'CI', 'start': 17, 'end': 19}, {'entity': 'I-ic', 'score': 0.9980363, 'index': 7, 'word': 'BI', 'start': 19, 'end': 21}, {'entity': 'I-ic', 'score': 0.9980369, 'index': 8, 'word': 'SK', 'start': 21, 'end': 23}, {'entity': 'I-ic', 'score': 0.9980236, 'index': 9, 'word': 'UP', 'start': 23, 'end': 25}, {'entity': 'I-ic', 'score': 0.9980083, 'index': 10, 'word': 'SKÃģ', 'start': 25, 'end': 28}, {'entity': 'I-ic', 'score': 0.99798703, 'index': 11, 'word': 'ĠKNI', 'start': 29, 'end': 32}, {'entity': 'I-ic', 'score': 0.9979876, 'index': 12, 'word': 'H', 'start': 32, 'end': 33}, {'entity': 'I-ic', 'score': 0.99799407, 'index': 13, 'word': 'Ġ-', 'start': 34, 'end': 35}, {'entity': 'I-ic', 'score': 0.99796957, 'index': 14, 'word': 'ĠA', 'start': 36, 'end': 37}, {'entity': 'I-ic', 'score': 0.99798775, 'index': 15, 'word': 'ĠKAM', 'start': 38, 'end': 41}, {'entity': 'I-ic', 'score': 0.99802506, 'index': 16, 'word': 'EN', 'start': 41, 'end': 43}, {'entity': 'I-ic', 'score': 0.9980363, 'index': 17, 'word': 'O', 'start': 43, 'end': 44}, {'entity': 'I-ic', 'score': 0.9980275, 'index': 18, 'word': 'TI', 'start': 44, 'end': 46}, {'entity': 'I-ic', 'score': 0.9979949, 'index': 19, 'word': 'SKÃģ', 'start': 46, 'end': 49}, {'entity': 'I-ic', 'score': 0.9979539, 'index': 20, 'word': 'RNA', 'start': 49, 'end': 52}, {'entity': 'I-ic', 'score': 0.9963624, 'index': 21, 'word': 'ĠV', 'start': 53, 'end': 54}, {'entity': 'B-gu', 'score': 0.55372596, 'index': 22, 'word': 'ĠO', 'start': 55, 'end': 56}, {'entity': 'I-gu', 'score': 0.99724305, 'index': 23, 'word': 'LOM', 'start': 56, 'end': 59}, {'entity': 'I-gu', 'score': 0.9976421, 'index': 24, 'word': 'OU', 'start': 59, 'end': 61}, {'entity': 'I-gu', 'score': 0.99610955, 'index': 25, 'word': 'CI', 'start': 61, 'end': 63}, {'entity': 'I-ic', 'score': 0.8969025, 'index': 37, 'word': 'ÄļJ', 'start': 88, 'end': 90}, {'entity': 'I-ic', 'score': 0.97333705, 'index': 38, 'word': 'SKÃī', 'start': 90, 'end': 93}, {'entity': 'I-ic', 'score': 0.99668163, 'index': 39, 'word': 'ĠBO', 'start': 94, 'end': 96}, {'entity': 'I-ic', 'score': 0.9979279, 'index': 40, 'word': 'HO', 'start': 96, 'end': 98}, {'entity': 'I-ic', 'score': 0.9979121, 'index': 41, 'word': 'SL', 'start': 98, 'end': 100}, {'entity': 'I-ic', 'score': 0.9979068, 'index': 42, 'word': 'OV', 'start': 100, 'end': 102}, {'entity': 'I-ic', 'score': 0.9979068, 'index': 43, 'word': 'EC', 'start': 102, 'end': 104}, {'entity': 'I-ic', 'score': 0.9978612, 'index': 44, 'word': 'KÃī', 'start': 104, 'end': 106}, {'entity': 'I-ic', 'score': 0.997883, 'index': 45, 'word': 'ĠFA', 'start': 107, 'end': 109}, {'entity': 'I-ic', 'score': 0.99782664, 'index': 46, 'word': 'KUL', 'start': 109, 'end': 112}, {'entity': 'I-ic', 'score': 0.99756145, 'index': 47, 'word': 'TY', 'start': 112, 'end': 114}]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "str = \"OLOMOUC 1920 . ARCIBISKUPSKÁ KNIH - A KAMENOTISKÁRNA V OLOMOUCI . NÁKLADEM CYRILLOMETHODĚJSKÉ BOHOSLOVECKÉ FAKULTY .\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"stulcrad/CNEC_2_0_robeczech-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stulcrad/CNEC_2_0_robeczech-base\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "results = classifier(str)\n",
    "combined__ = combine_adjacent_name_tokens(results)\n",
    "\n",
    "print(results)\n",
    "print(combined__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-ner-env-py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
