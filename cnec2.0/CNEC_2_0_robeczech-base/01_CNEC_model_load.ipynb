{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "classifier = pipeline(\"ner\", model=\"stulcrad/CNEC_2_0_robeczech-base\")\n",
    "str_ = \"Jste světa znalý muž a víte stejně dobře jako já , že souvislost mezi současnými krutostmi v Jihovýchodní Asii a tou novou bankovní pobočkou hned vedle obchoďáku Zátoka je přímá a bezprostřední ; byl z toho už vzteklý jak uvázaný pes , protože zájemci o hodiny mu úplně narušili jeho denní režim a on si nemohl po obědě ani zdřímnout .\"\n",
    "classifier(str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_[162:168]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "def combine_adjacent_name_tokens(results):\n",
    "    \"\"\"\n",
    "    Spojí navazující tokeny jmen do celých jmen.\n",
    "    \n",
    "    Args:\n",
    "        results: Seznam tokenů z NER modelu.\n",
    "    \n",
    "    Returns:\n",
    "        Seznam slovníků obsahujících spojená jména.\n",
    "    \"\"\"\n",
    "    # Typy tokenů pro jména\n",
    "    person_types = ['pf', 'ps', 'pp', 'pm', 'pc', 'pd' ]\n",
    "    \n",
    "    combined_names = []\n",
    "    current_name = None\n",
    "    last_index = -1\n",
    "    \n",
    "    # Seřadit výsledky podle indexu\n",
    "    sorted_results = sorted(results, key=lambda x: x['index'])\n",
    "    \n",
    "    for token in sorted_results:\n",
    "        # Kontrola, zda je token část jména\n",
    "        entity_type = token['entity'].split('-')[1] if '-' in token['entity'] else None\n",
    "        is_name_part = entity_type in person_types\n",
    "        \n",
    "        if is_name_part:\n",
    "            if current_name is None or token['index'] != last_index + 1:\n",
    "                # Začít nové jméno, pokud nejde o následující token\n",
    "                if current_name:\n",
    "                    combined_names.append(current_name)\n",
    "                current_name = {\n",
    "                    'text': token['word'].lstrip('Ġ'),  # odstranit prefix tokenizéru\n",
    "                    'start': token['start'],\n",
    "                    'end': token['end'],\n",
    "                    'tokens': [token]\n",
    "                }\n",
    "            else:\n",
    "                # Přidat k současnému jménu - speciální zpracování pro interpunkci\n",
    "                if token['word'] == 'Ġ.':\n",
    "                    current_name['text'] += '.'  # bez mezery pro tečku\n",
    "                else:\n",
    "                    # Pro ostatní tokeny\n",
    "                    current_name['text'] += token['word'].lstrip('Ġ')\n",
    "                current_name['end'] = token['end']\n",
    "                current_name['tokens'].append(token)\n",
    "            \n",
    "            last_index = token['index']\n",
    "    \n",
    "    # Přidat poslední jméno\n",
    "    if current_name:\n",
    "        combined_names.append(current_name)\n",
    "        \n",
    "    return combined_names\n",
    "\n",
    "def extract_and_mark_names(text, results):\n",
    "    \"\"\"\n",
    "    Ohraničí nalezená jména značkami @@ a ##.\n",
    "    \n",
    "    Args:\n",
    "        text: Původní text.\n",
    "        results: Výstupy z NER modelu.\n",
    "    \n",
    "    Returns:\n",
    "        Text s označenými jmény.\n",
    "    \"\"\"\n",
    "    # Získáme pozice všech jmen\n",
    "    combined = combine_adjacent_name_tokens(results)\n",
    "    \n",
    "    # Seřadíme podle začátku\n",
    "    combined.sort(key=lambda x: x['start'])\n",
    "    \n",
    "    # Vytvoříme nový text s označenými jmény\n",
    "    marked_text = \"\"\n",
    "    last_end = 0\n",
    "    \n",
    "    for name in combined:\n",
    "        # Přidáme text před jménem\n",
    "        marked_text += text[last_end:name['start']]\n",
    "        \n",
    "        # Přidáme označené jméno\n",
    "        original_name = text[name['start']:name['end']]\n",
    "        marked_text += \"@@\" + original_name + \"##\"\n",
    "        \n",
    "        # Aktualizujeme pozici\n",
    "        last_end = name['end']\n",
    "    \n",
    "    # Přidáme zbývající text po posledním jménu\n",
    "    marked_text += text[last_end:]\n",
    "    \n",
    "    return marked_text\n",
    "\n",
    "def process_dataset_with_model(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Zpracuje dataset pomocí modelu a uloží označené kontexty do výstupního souboru.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Cesta k vstupnímu JSON souboru.\n",
    "        output_file: Cesta k výstupnímu textovému souboru.\n",
    "    \"\"\"\n",
    "    # Načtení modelu\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"stulcrad/CNEC_2_0_robeczech-base\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"stulcrad/CNEC_2_0_robeczech-base\")\n",
    "    classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    # Načtení datasetu\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Zpracování a uložení výsledků\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            context = item[\"context\"]\n",
    "            results = classifier(context)\n",
    "            marked_context = extract_and_mark_names(context, results)\n",
    "            f.write(marked_context + '\\n')\n",
    "\n",
    "# Hlavní funkce\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"input_dataset.json\"\n",
    "    output_file = \"hokuspokus.txt\"\n",
    "    process_dataset_with_model(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-pf', 'score': 0.9994073, 'index': 7, 'word': 'ĠFrantiÅ¡ka', 'start': 19, 'end': 28}, {'entity': 'B-ps', 'score': 0.9985337, 'index': 8, 'word': 'ĠHala', 'start': 29, 'end': 33}, {'entity': 'I-ps', 'score': 0.99907947, 'index': 9, 'word': 'se', 'start': 33, 'end': 35}, {'entity': 'B-oa', 'score': 0.99799675, 'index': 10, 'word': 'ĠNaÅ¡e', 'start': 36, 'end': 40}, {'entity': 'I-oa', 'score': 0.9988361, 'index': 11, 'word': 'ĠpanÃŃ', 'start': 41, 'end': 45}, {'entity': 'I-oa', 'score': 0.9988016, 'index': 12, 'word': 'ĠBoÅ¾ena', 'start': 46, 'end': 52}, {'entity': 'I-oa', 'score': 0.9984163, 'index': 13, 'word': 'ĠNÄĽmcovÃ¡', 'start': 53, 'end': 60}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "str = \"Zasáhli u pořadu z Františka Halase Naše paní Božena Němcová , což bylo v dobách prehistorie souboru .\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"stulcrad/CNEC_2_0_robeczech-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stulcrad/CNEC_2_0_robeczech-base\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "results = classifier(str)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-ner-env-py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
