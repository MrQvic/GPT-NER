{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "classifier = pipeline(\"ner\", model=\"stulcrad/CNEC_2_0_robeczech-base\")\n",
    "str_ = \"Jste světa znalý muž a víte stejně dobře jako já , že souvislost mezi současnými krutostmi v Jihovýchodní Asii a tou novou bankovní pobočkou hned vedle obchoďáku Zátoka je přímá a bezprostřední ; byl z toho už vzteklý jak uvázaný pes , protože zájemci o hodiny mu úplně narušili jeho denní režim a on si nemohl po obědě ani zdřímnout .\"\n",
    "classifier(str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_[162:168]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\GitHub\\GPT-NER\\gpt-ner-env-py38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "def combine_adjacent_name_tokens(results):\n",
    "    \"\"\"\n",
    "    Spojí navazující tokeny jmen do celých jmen.\n",
    "    \n",
    "    Args:\n",
    "        results: Seznam tokenů z NER modelu.\n",
    "    \n",
    "    Returns:\n",
    "        Seznam slovníků obsahujících spojená jména.\n",
    "    \"\"\"\n",
    "    # Typy tokenů pro jména\n",
    "    person_types = ['pf', 'ps', 'pp', 'pm', 'pc', 'pd' ]\n",
    "    \n",
    "    combined_names = []\n",
    "    current_name = None\n",
    "    last_index = -1\n",
    "    \n",
    "    # Seřazení výsledků podle indexu\n",
    "    sorted_results = sorted(results, key=lambda x: x['index'])\n",
    "    \n",
    "    for token in sorted_results:\n",
    "        # Rozdělení entity na typ\n",
    "        entity_type = token['entity'].split('-')[1] if '-' in token['entity'] else \"\"\n",
    "        if entity_type in person_types:\n",
    "            if current_name is None or token['index'] != last_index + 1:\n",
    "                # Nové jméno\n",
    "                if current_name:\n",
    "                    combined_names.append(current_name)\n",
    "                current_name = {\n",
    "                    'start': token['start'],\n",
    "                    'end': token['end']\n",
    "                }\n",
    "            else:\n",
    "                # Přidání dalšího tokenu\n",
    "                current_name['end'] = token['end']\n",
    "            \n",
    "            last_index = token['index']\n",
    "    \n",
    "    # Přidání posledního jména\n",
    "    if current_name:\n",
    "        combined_names.append(current_name)\n",
    "        \n",
    "    return combined_names\n",
    "\n",
    "def extract_and_mark_names(text, results):\n",
    "    \"\"\"\n",
    "    Ohraničí nalezená jména značkami @@ a ##.\n",
    "    \n",
    "    Args:\n",
    "        text: Původní text.\n",
    "        results: Výstupy z NER modelu.\n",
    "    \n",
    "    Returns:\n",
    "        Text s označenými jmény.\n",
    "    \"\"\"\n",
    "    \n",
    "    combined = combine_adjacent_name_tokens(results)\n",
    "    combined.sort(key=lambda x: x['start'])\n",
    "    \n",
    "    marked_text = \"\"\n",
    "    last_end = 0\n",
    "    \n",
    "    # Přidání @@## k nalezeným jménům\n",
    "    for name in combined:\n",
    "        marked_text += text[last_end:name['start']]\n",
    "        \n",
    "        original_name = text[name['start']:name['end']]\n",
    "        marked_text += \"@@\" + original_name + \"##\"\n",
    "        \n",
    "        last_end = name['end']\n",
    "    \n",
    "    marked_text += text[last_end:]\n",
    "    \n",
    "    return marked_text\n",
    "\n",
    "def process_dataset_with_model(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Zpracuje dataset pomocí modelu a uloží označené kontexty do výstupního souboru.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Cesta k vstupnímu JSON souboru.\n",
    "        output_file: Cesta k výstupnímu textovému souboru.\n",
    "    \"\"\"\n",
    "    # Načtení modelu\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"stulcrad/CNEC_2_0_robeczech-base\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"stulcrad/CNEC_2_0_robeczech-base\")\n",
    "    classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    # Načtení datasetu\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Zpracování a uložení\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            context = item[\"context\"]\n",
    "            results = classifier(context)\n",
    "            marked_context = extract_and_mark_names(context, results)\n",
    "            f.write(marked_context + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"input_dataset.json\"\n",
    "    output_file = \"hokuspokus.txt\"\n",
    "    process_dataset_with_model(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-gc', 'score': 0.99654895, 'index': 10, 'word': 'ĠIzraelem', 'start': 31, 'end': 39}, {'entity': 'B-pp', 'score': 0.9708674, 'index': 12, 'word': 'ĠBohem', 'start': 42, 'end': 47}, {'entity': 'B-pp', 'score': 0.9767138, 'index': 28, 'word': 'ĠBoha', 'start': 113, 'end': 117}, {'entity': 'B-pp', 'score': 0.9637986, 'index': 34, 'word': 'ĠJeÅ¾ÃŃÅ¡e', 'start': 138, 'end': 144}, {'entity': 'B-pp', 'score': 0.9630684, 'index': 48, 'word': 'ĠBohem', 'start': 188, 'end': 193}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "str = \"Polovičatost modu vivendi mezi Izraelem a Bohem , placená zamlžováním boží pravdy a lidského hříchu , znevážením Boha i člověka , jitřila Ježíše tak , že se rozhodl nemlčet a handlování s Bohem skončit , ve skutečnosti však experiment s jeho všemohoucností vyostřit .\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"stulcrad/CNEC_2_0_robeczech-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stulcrad/CNEC_2_0_robeczech-base\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "results = classifier(str)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-ner-env-py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
