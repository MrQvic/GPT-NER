{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import urllib\n",
    "\n",
    "available_annotations = sorted(Path(\".\").glob(\"project-42-at-*.json\"))\n",
    "latest_annotation = available_annotations[-1]\n",
    "#ls_tasks = json.loads(latest_annotation.read_text())\n",
    "ls_tasks = json.loads(latest_annotation.read_text(encoding='utf-8'))\n",
    "\n",
    "def extract_annotations_form_task(task: list[dict]):\n",
    "    annotations = []\n",
    "    for task in ls_tasks:\n",
    "        url = urllib.parse.urlparse(task[\"data\"][\"text\"])\n",
    "        query = urllib.parse.parse_qs(url.query)\n",
    "        d= urllib.parse.unquote(query['d'][0]) \n",
    "        data_path = Path(d)\n",
    "        for annotation in task[\"annotations\"]:\n",
    "            for result in annotation[\"result\"]:\n",
    "                if result['from_name'] != 'label' or result['to_name'] != 'text':\n",
    "                    continue\n",
    "                value = result['value']\n",
    "                annotations.append({\n",
    "                    'task_id': task[\"id\"],\n",
    "                    'annotation_id' : annotation[\"id\"],\n",
    "                    'result_id' : result[\"id\"],\n",
    "                    'file_name': data_path.name,\n",
    "                    'data_split' : data_path.parent.name,\n",
    "                    'data_source' : data_path.parent.parent.name,\n",
    "                    'start_char' : value['start'],\n",
    "                    'end_char': value['end'],\n",
    "                    'label' : value['labels'][0],\n",
    "                })\n",
    "    return pd.DataFrame(annotations)\n",
    "    \n",
    "\n",
    "def load_data_zip(path_to_data: Path):\n",
    "    import zipfile\n",
    "    import io\n",
    "    import pandas as pd\n",
    "    data = []\n",
    "    with zipfile.ZipFile(path_to_data, 'r') as zip_ref:\n",
    "        files = zip_ref.namelist()\n",
    "        for file in files:\n",
    "            file_path = Path(file)\n",
    "            with zip_ref.open(file) as f:\n",
    "                data.append({\n",
    "                    'file_name': file_path.name,\n",
    "                    'data_split' : file_path.parent.name,\n",
    "                    'data_source' : file_path.parent.parent.name,\n",
    "                    'text' : f.read().decode('utf-8')\n",
    "                })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def convert_char_positions_to_word_positions(result_id: str, start_char: int, end_char: int, word_mapping: np.ndarray):\n",
    "    return {\n",
    "        \"result_id\" : result_id,\n",
    "        \"start_word\" : word_mapping[start_char],\n",
    "        \"end_word\" : word_mapping[end_char - 1]\n",
    "    }\n",
    "\n",
    "\n",
    "def get_word_mapping(text: str, per_line: bool = False):\n",
    "    isword = False\n",
    "    word_index = -1\n",
    "    word_mapping = []\n",
    "    words = []\n",
    "    for char in text:\n",
    "        if char.isspace() and (char != '\\n' or not per_line):\n",
    "            word_mapping.append(word_index)\n",
    "            isword = False\n",
    "        elif char == '\\n' and per_line:\n",
    "            word_mapping.append(word_index)\n",
    "            isword = False\n",
    "            word_index = -1\n",
    "        elif char in [\".\", \",\", \"!\", \"?\", \":\", \";\", '-', '(', ')', '[', ']', '{', '}', '<', '>', '\"', \"'\", \"`\", \"’\", \"‘\", \"“\", \"”\", \"„\", \"‟\", \"‛\", \"‟\", \"‹\", \"›\", \"«\", \"»\", \"—\", \"=\"]:\n",
    "            word_index += 1\n",
    "            word_mapping.append(word_index)\n",
    "            words.append(char)\n",
    "            isword = False\n",
    "        else:\n",
    "            if isword:\n",
    "                word_mapping.append(word_index)\n",
    "                words[-1] += char\n",
    "            else:\n",
    "                isword = True\n",
    "                word_index += 1\n",
    "                word_mapping.append(word_index)\n",
    "                words.append(char)\n",
    "    word_mapping = np.array(word_mapping, dtype=int)\n",
    "    assert len(word_mapping) == len(text)\n",
    "    return ' '.join(words), word_mapping\n",
    "\n",
    "\n",
    "data = load_data_zip(Path(\"NER_xmrkva04.zip\"))\n",
    "data[['word_text', 'word_mapping']] = data['text'].apply(lambda x: pd.Series(get_word_mapping(x, per_line=False)))\n",
    "data = data.set_index(['file_name', 'data_split', 'data_source'])\n",
    "ls_annotations = extract_annotations_form_task(ls_tasks)\n",
    "converted_start_ends = []\n",
    "for i, row in ls_annotations.iterrows():\n",
    "    data_row = data.loc[(row['file_name'], row['data_split'], row['data_source'])]\n",
    "    converted_start_ends.append(convert_char_positions_to_word_positions(\n",
    "        row['result_id'], row['start_char'], row['end_char'], data_row['word_mapping']))\n",
    "converted_start_ends = pd.DataFrame(converted_start_ends)\n",
    "ls_annotations = ls_annotations.merge(converted_start_ends, on='result_id')\n",
    "ls_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tasks: 3075\n"
     ]
    }
   ],
   "source": [
    "unique_task_count = ls_annotations['task_id'].nunique()\n",
    "print(f\"Number of unique tasks: {unique_task_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_char_html(text: str, annotations: pd.DataFrame):\n",
    "    tag_list = [[] for _ in range(len(text))]\n",
    "    start_char_tag = '<span style=\"color: red\">'\n",
    "    end_char_tag = '</span>'\n",
    "    for i, row in annotations.iterrows():\n",
    "        tag_list[row['start_char']].append(start_char_tag)\n",
    "        tag_list[row['end_char']].insert(0,end_char_tag)\n",
    "    for i in range(len(tag_list)):\n",
    "        tag_list[i] = ''.join(tag_list[i]) + text[i]\n",
    "    return ''.join(tag_list)\n",
    "\n",
    "def to_word_html(text: str, annotations: pd.DataFrame):\n",
    "    words = text.split(' ')\n",
    "    tag_list = [[] for _ in range(len(words) + 1)]\n",
    "    start_char_tag = '<span style=\"color: red\">'\n",
    "    end_char_tag = '</span>'\n",
    "    for i, row in annotations.iterrows():\n",
    "        tag_list[row['start_word']].append(start_char_tag)\n",
    "        tag_list[row['end_word'] + 1].insert(0, end_char_tag)\n",
    "    for i in range(len(words)):\n",
    "        tag_list[i] = ''.join(tag_list[i]) + words[i]\n",
    "    tag_list[-1] = ''.join(tag_list[-1])\n",
    "    return ' '.join(tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "entities_map = {\n",
    "    'per' : 'PER'\n",
    "}\n",
    "\n",
    "\n",
    "for (file_name, data_split, data_source, label), annotations_df in ls_annotations[ls_annotations['label'].isin(entities_map.keys())].sort_values(by=['file_name']).groupby(['file_name', 'data_split', 'data_source', 'label']):\n",
    "    data_row = data.loc[(file_name, data_split, data_source)]\n",
    "    display(HTML(f\"<h4>{entities_map[label]} - Chars</h4>\" +\n",
    "            to_char_html(data_row['text'], annotations_df).replace('\\n', '<br>')))\n",
    "    display(HTML(f\"<h4>{entities_map[label]} - Words</h4>\" +\n",
    "            to_word_html(data_row['word_text'], annotations_df).replace('\\n', '<br>')))\n",
    "    c += 1\n",
    "    if c > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_ner_fmt = []\n",
    "\n",
    "doc_id = 0\n",
    "per_id = 1\n",
    "for (file_name, data_split, data_source, label), annotations_df in ls_annotations[ls_annotations['label'] == 'per'].sort_values(by=['file_name']).groupby(['file_name', 'data_split', 'data_source', 'label']):\n",
    "    data_row = data.loc[(file_name, data_split, data_source)]\n",
    "    gpt_ner_fmt .append({\n",
    "        'context': data_row['word_text'],\n",
    "        \"end_position\" : [int(x) for x in annotations_df['end_word']],\n",
    "        \"entity_label\" : entities_map[label],\n",
    "        \"impossible\" : len(annotations_df) == 0,\n",
    "        \"qas_id\" : f\"{doc_id}.{per_id}\",\n",
    "        \"query\": \"person entities are named persons or family.\",\n",
    "        \"span_position\": [f'{int(x[\"start_word\"])};{int(x[\"end_word\"])}' for _, x in annotations_df[['start_word', 'end_word']].iterrows()],\n",
    "        \"start_position\" : [int(x) for x in annotations_df['start_word']],\n",
    "        \"data_source\" : data_source,\n",
    "        \"file_name\" : file_name,\n",
    "        \"anotator_id\" : data_split\n",
    "    })\n",
    "\n",
    "gpt_ner_fmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_ner_fmt = []\n",
    "\n",
    "doc_id = 0\n",
    "\n",
    "# Group without filtering\n",
    "for (file_name, data_split, data_source), file_annotations in ls_annotations.sort_values(by=['file_name']).groupby(['file_name', 'data_split', 'data_source']):\n",
    "    data_row = data.loc[(file_name, data_split, data_source)]\n",
    "    \n",
    "    # Get only person annotations\n",
    "    per_annotations = file_annotations[file_annotations['label'] == 'per']\n",
    "    \n",
    "    gpt_ner_fmt.append({\n",
    "        'context': data_row['word_text'],\n",
    "        \"end_position\": [int(x) for x in per_annotations['end_word']] if not per_annotations.empty else [],\n",
    "        \"entity_label\": entities_map['per'] if not per_annotations.empty else entities_map['per'],\n",
    "        \"impossible\": len(per_annotations) == 0,\n",
    "        \"qas_id\": f\"{doc_id}.{1}\",\n",
    "        \"query\": \"person entities are named persons or family.\",\n",
    "        \"span_position\": [f'{int(x[\"start_word\"])};{int(x[\"end_word\"])}' for _, x in per_annotations[['start_word', 'end_word']].iterrows()] if not per_annotations.empty else [],\n",
    "        \"start_position\": [int(x) for x in per_annotations['start_word']] if not per_annotations.empty else [],\n",
    "        \"data_source\": data_source,\n",
    "        \"file_name\": file_name,\n",
    "        \"anotator_id\": data_split\n",
    "    })\n",
    "    \n",
    "    doc_id += 1\n",
    "\n",
    "gpt_ner_fmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Historical-NER-Dataset_gpt_ner_fmt_FULL_42.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(gpt_ner_fmt, f, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
